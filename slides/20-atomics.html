<!-- ATOMICS ARE SLOW -->
<section id="atomics">
  <section class="section-header">
    <h2>Atomics are Slow</h2>
    <p>Avoid them, batch them, or skip them entirely</p>
  </section>

  <section>
    <h3>The Cost of Atomics</h3>
    <table class="benchmark">
      <thead>
        <tr><th>Operation</th><th>Plain</th><th>Atomics</th><th>Slowdown</th></tr>
      </thead>
      <tbody>
        <tr>
          <td>read i32</td>
          <td>0.27 ns</td>
          <td>Atomics.load: 8.5 ns</td>
          <td><strong>31x slower</strong></td>
        </tr>
        <tr>
          <td>write i32</td>
          <td>1.3 ns</td>
          <td>Atomics.store: 9.5 ns</td>
          <td><strong>7x slower</strong></td>
        </tr>
      </tbody>
    </table>
    <div class="insight">
      Every Atomics call is a memory fence; flushes store buffers and synchronizes cache lines.
      V8 is conservative: TurboFan (the JIT) never optimizes, reorders, or elides Atomics, unlike C++ compilers.
    </div>
  </section>

  <section>
    <h3>Batch, Don't Spam</h3>
    <pre><code class="language-javascript">// BAD: Atomics.store on every write
for (let i = 0; i < 100; i++) {
  Atomics.store(i32, 0, i)     // 100 × memory fence = 840 ns
}

// GOOD: plain writes + single atomic publish
for (let i = 0; i < 100; i++) {
  i32[0] = i                   // plain write, no fence
}
Atomics.store(i32, 0, i32[0])  // 1 × memory fence = 39 ns total</code></pre>
    <table class="benchmark">
      <thead><tr><th>Pattern</th><th>Time</th><th>Speedup</th></tr></thead>
      <tbody>
        <tr><td>100 × Atomics.store</td><td>840 ns</td><td>—</td></tr>
        <tr><td>100 × plain + 1 × Atomics.store</td><td>39 ns</td><td><strong>21x faster</strong></td></tr>
      </tbody>
    </table>
    <div class="insight">
      This is what @nxtedition/shared does: plain writes into the ring buffer,
      single Atomics.store to publish the new write position. Corking batches even further.
    </div>
  </section>

  <section>
    <h3>Real-World: writer.cork()</h3>
    <pre><code class="language-javascript">// @nxtedition/shared – cork batches N writes into 1 atomic publish
writer.cork(() => {
  for (let i = 0; i < 100; i++) {
    writer.writeSync(buf.byteLength, ({ buffer, byteOffset }) =>        // plain memcpy, no fence
      byteOffset + buf.copy(buffer, byteOffset)
    )
  }
})
// on uncork → single Atomics.store(state, WRITE_INDEX, writePos)</code></pre>
  </section>

  <section>
    <h3>Polling vs wait/notify</h3>
    <pre><code class="language-javascript">// Option 1: Atomics.wait
Atomics.wait(i32, 0, oldValue)  // block until value changes
// Zero CPU, but OS scheduling adds µs of wakeup latency

// Option 2: Atomics.pause spin-wait, burns CPU
while (Atomics.load(i32, 0) === oldValue) {
  Atomics.pause() // hint to CPU we're in a spin loop, reduces power but still burns CPU
}

// Option 3: timer polling
const poll = setInterval(() => {
  if (i32[0] !== oldValue) { clearInterval(poll); onData() }
}, 1)</code></pre>
  </section>

  <section>
    <h3>On x86: Atomics can be Unnecessary</h3>
    <pre><code class="language-javascript">// x86 Total Store Order (TSO):
// - 32-bit reads and writes are atomic by hardware
// - Stores are visible to other cores in program order

// So for SPSC queues, plain access works:
i32[IDX] = newPos          // instead of Atomics.store()
const pos = i32[IDX]       // instead of Atomics.load()

// Caveat: V8/TurboFan can reorder plain accesses!
// Atomics act as a compiler fence, preventing V8 from moving reads/writes past it.

// @nxtedition/shared uses exactly this:
// plain writes for data, plain reads to poll,
// single Atomics.store to publish write position (compiler + HW fence)</code></pre>
    <div class="warning">
      Platform-specific &amp; outside the JS spec.
      Safe on x86 and ARM64 (all Node.js platforms). Not portable to weaker memory models.
    </div>
  </section>
</section>
