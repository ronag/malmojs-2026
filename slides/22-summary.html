<!-- SUMMARY & TAKEAWAYS -->
<section>
  <section class="section-header">
    <h2>Key Takeaways</h2>
  </section>

  <section>
    <h3>The Performance Mindset</h3>
    <ol class="spaced" style="font-size: 0.65em;">
      <li><strong>Measure first</strong> — don't guess where bottlenecks are; optimize only hot paths or high-scale workloads</li>
      <li><strong>Don't trust benchmarks blindly</strong> — V8 eliminates dead code and folds constants; CPU predicts branches and saturates caches; use <code>do_not_optimize</code>, realistic data sizes, GC pressure; always validate against production</li>
      <li><strong>Reduce allocations</strong> — every object is a GC promise; use opaque callbacks, intrusive containers, Slice over Buffer.subarray, pool allocators; prefer cache-friendly structures — O(n) sequential often beats O(1) with pointer chasing</li>
      <li><strong>Avoid unnecessary async</strong> — sync fast paths beat async by 145×; use the async return pattern <code>{async, value}</code> for conditionally async APIs</li>
      <li><strong>Pool &amp; bound expensive resources</strong> — timers, buffers, connections; always cap pool size to avoid major GC pressure</li>
      <li><strong>Yield cooperatively</strong> — <code>await</code> drains microtasks, not the event loop; use <code>setImmediate</code>/<code>maybeYield</code> to stay responsive</li>
      <li><strong>Schedule with priorities</strong> — not all work is equal; defer background tasks; coordinate UV thread pool across workers</li>
      <li><strong>Use shared memory</strong> — ring buffer over MessagePort (11× faster batched); batch plain writes, single <code>Atomics.store</code> to publish</li>
      <li><strong>Pack state into bitmaps</strong> — replace boolean properties with bit flags; test multiple flags in one bitwise AND; counters in LSBs</li>
      <li><strong>Avoid Web API compatibility tax</strong> — <code>request-target</code> over <code>new URL()</code> (9×); <code>undici</code> over <code>fetch</code> (3×); Node Streams over Web Streams for writes (13×)</li>
      <li><strong>Pick the right concurrency model</strong> — stay single-threaded for &lt;1 ms CPU; async for I/O-bound; workers only when CPU work exceeds ~40 ms</li>
      <li><strong>Prefer Workers + reusePort over cluster</strong> — OS kernel load balancing, no primary bottleneck, shared memory via SharedArrayBuffer</li>
      <li><strong>Configure your runtime</strong> — UV thread pool size, <code>--max-semi-space-size</code>, <code>--max-old-space-size</code>, <code>Buffer.poolSize</code>, <code>stream.defaultHighWaterMark</code></li>
    </ol>
  </section>

  <section>
    <h3>Libraries Used</h3>
    <table class="benchmark" style="font-size: 0.7em;">
      <thead>
        <tr><th>Package</th><th>Purpose</th></tr>
      </thead>
      <tbody>
        <tr><td>@nxtedition/timers</td><td>Pooled timer management</td></tr>
        <tr><td>@nxtedition/yield</td><td>Cooperative event loop yielding</td></tr>
        <tr><td>@nxtedition/scheduler</td><td>Priority-based task scheduling</td></tr>
        <tr><td>@nxtedition/slice</td><td>Zero-alloc buffer management</td></tr>
        <tr><td>@nxtedition/shared</td><td>Lock-free cross-thread ring buffer</td></tr>
      </tbody>
    </table>
  </section>

  <section>
    <h2>Thank You!</h2>
    <p>Malmö JS 2026</p>
    <p class="small">Slides & code: <a href="https://github.com/ronag/malmojs-2026">github.com/ronag/malmojs-2026</a></p>
  </section>
</section>
